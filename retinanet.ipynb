{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apekcRFCb_Tx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "from six import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tlOCW8usjM-"
   },
   "outputs": [],
   "source": [
    "def convert_to_xywh(boxes):\n",
    "  return tf.concat([(boxes[...,:2]+boxes[...,2:])/2.0,boxes[...,2]-boxes[...,:2]],axis=-1,)\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "  return tf.concat([boxes[...,:2]-boxes[...,2:]/2.0,boxes[...,:2]+boxes[...,2:]/2.0],axis=-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEE6npl6tkJd"
   },
   "outputs": [],
   "source": [
    "def compute_iou(boxes1,boxes2):\n",
    "  boxes1_corners=convert_to_corners(boxes1)\n",
    "  boxes2_corners=convert_to_corners(boxes2)\n",
    "  lu=tf.maximum(boxes1_corners[:,None,:2],boxes2_corners[:,:2])\n",
    "  rd=tf.minimum(boxes1_corners[:,None,2:],boxes2_corners[:,2:])\n",
    "  intersection=tf.maximum(0.0,rd-lu)\n",
    "  intersection_area=intersection[:,:,0]*intersection[:,:,1]\n",
    "  boxes1_area=boxes1[:,2]*boxes1[:,3]\n",
    "  boxes2_area=boxes2[:,2]*boxes2[:,3]\n",
    "  union_area=tf.maximum(boxes1_area[:,None]+boxes2_area-intersection_area,1e-8)\n",
    "  return tf.clip_by_value(intersection_area/union_area,0.0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQspi_uKcRll"
   },
   "outputs": [],
   "source": [
    "def visualize_detections(image,boxes,classes,scores,figsize=(7,7),linewidth=1,color=[0,0,1]):\n",
    "  image=np.array(image,dtype=np.uint8)\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.axis(\"off\")\n",
    "  plt.imshow(image)\n",
    "  ax=plt.gca()\n",
    "  for box,_cls,score in zip(boxes,classes,scores):\n",
    "    test=\"{}:{:.2f}\".format(_cls,score)\n",
    "    x1,y1,x2,y2=box\n",
    "    w,h=x2-x1,y2-y1\n",
    "    patch=plt.Rectangle([x1,y1],w,h,fill=False,edgecolor=color,linewidth=linwidth)\n",
    "    ax.add_patch(patch)\n",
    "    ax.text(x1,y1,text,bbox={\"facecolor\":color,\"alpha\":0.4},clip_box=ax.clipbox,clip_on=True)\n",
    "  plt.show()\n",
    "  return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0GqQosPeJQy"
   },
   "outputs": [],
   "source": [
    "class AnchorBox:\n",
    "  def __init__(self):\n",
    "    self.aspect_ratios=[0.5,1.0,2.0]\n",
    "    self.scales=[2**x for x in [0,1/3,2/3]]\n",
    "\n",
    "    self._num_anchors=len(self.aspect_ratios)*len(self.scales)\n",
    "    self._strides=[2**i for i in range(3,8)]\n",
    "    self._areas=[x**2 for x in [32.0,64.0,128.0,256.0,512.0]]\n",
    "    self._anchor_dims=self._compute_dims()\n",
    "  \n",
    "  def _compute_dims(self):\n",
    "    anchor_dims_all=[]\n",
    "    for area in self._areas:\n",
    "      anchor_dims=[]\n",
    "      for ratio in self.aspect_ratios:\n",
    "        anchor_height=tf.math.sqrt(area/ratio)\n",
    "        anchor_width=area/anchor_height\n",
    "        dims=tf.reshape(tf.stack([anchor_width,anchor_height],axis=-1),[1,1,2])\n",
    "\n",
    "        for scale in self.scales:\n",
    "          anchor_dims.append(scale*dims)\n",
    "      anchor_dims_all.append(tf.stack(anchor_dims,axis=-2))\n",
    "    return anchor_dims_all\n",
    "  \n",
    "  def _get_anchors(self,feature_height,feature_width,level):\n",
    "    rx=tf.range(feature_width,dtype=tf.float32)+0.5\n",
    "    ry=tf.range(feature_height,dtype=tf.float32)+0.5\n",
    "    centers=tf.stack(tf.meshgrid(rx,ry),axis=-1)*self._strides[level-3]\n",
    "    centers=tf.expand_dims(centers,axis=-2)\n",
    "    centers=tf.tile(centers,[1,1,self._num_anchors,1])\n",
    "    dims=tf.tile(self._anchor_dims[level-3],[feature_height,feature_width,1,1])\n",
    "    anchors=tf.concat([centers,dims],axis=-1)\n",
    "    return tf.reshape(anchors,[feature_height*feature_width*self._num_anchors,4])\n",
    "\n",
    "  def get_anchors(self,image_height,image_width):\n",
    "    anchors=[self._get_anchors(tf.math.ceil(image_height/2**i),tf.math.ceil(image_width/2**i),i,) for i in range(3,8)]\n",
    "    return tf.concat(anchors,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRNxDF4jgwMO"
   },
   "outputs": [],
   "source": [
    "# preprocessing data\n",
    "def random_flip_horizontal(image,boxes):\n",
    "  if tf.random.uniform(())>0.5:\n",
    "    image=tf.image.flip_left_right(image)\n",
    "    boxes=tf.stack([1-boxes[:,2],boxes[:,1],1-boxes[:,0],boxes[:,3]],axis=-1)\n",
    "  return image,boxes\n",
    "\n",
    "def resize_and_pad_image(image,min_sid=800.0,max_size=1333.0,jitter=[640,1024],stride=128.0):\n",
    "  image_shape=tf.cast(tf.shape(image)[:2],dtype=tf.float32)\n",
    "  if jitter is not None:\n",
    "    min_side=tf.random.uniform((),jitter[0],jitter[1],dtype=tf.float32)\n",
    "  ratio=min_side/tf.reduce_max(image_shape)\n",
    "  if ratio*tf.reduce_max(image_shape)>max_size:\n",
    "    ratio=max_size/tf.reduce_max(image_shape)\n",
    "  image=tf.image.resize(image,tf.cast(image_shape,dtype=tf.int32))\n",
    "  paddded_image_shape=tf.cast(tf.math.ceil(image_shape/stride)*stride,dtype=tf.int32)\n",
    "  image=tf.image.pad_to_bounding_box(image,0,0,paddded_image_shape[0],paddded_image_shape[1])\n",
    "  return image,image_shape,ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfzQ7mG0tCxz"
   },
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "  def __init__(self):\n",
    "    self._anchor_box=AnchorBox()\n",
    "    self._box_variance=tf.convert_to_tensor([0.1,0.1,0.2,0.2],dtype=tf.float32)\n",
    "\n",
    "  def _match_anchor_boxes(\n",
    "      self,anchor_boxes,gt_boxes,match_iou=0.5,ignore_iou=0.4\n",
    "  ):\n",
    "    iou_matrix=compute_iou(anchor_boxes,gt_boxes)\n",
    "    max_iou=tf.reduce_max(iou_matrix,axis=1)\n",
    "    matched_gt_idx=tf.argmax(iou_matrix,axis=1)\n",
    "    positive_mask=tf.greater_equal(max_iou,match_iou)\n",
    "    negative_mask=tf.less(max_iou,ignore_iou)\n",
    "    ignore_mask=tf.logical_not(tf.logical_or(positive_mask,negative_mask))\n",
    "    return (matched_gt_idx,tf.cast(positive_mask,dtype=tf.float32),tf.cast(ignore_mask,dtype=tf.float32),)\n",
    "\n",
    "  def _compute_box_target(self,anchor_boxes,matched_gt_boxes):\n",
    "    # print(anchor_boxes,matched_gt_boxes)\n",
    "    box_target=tf.concat([(matched_gt_boxes[:,:2]-anchor_boxes[:,:2])/anchor_boxes[:,2:],tf.math.log(matched_gt_boxes[:,2:]/anchor_boxes[:,2:]),],axis=-1,)\n",
    "    box_target=box_target/self._box_variance\n",
    "    return box_target\n",
    "  \n",
    "  def _encode_sample(self,image_shape,gt_boxes,cls_ids):\n",
    "    anchor_boxes=self._anchor_box.get_anchors(image_shape[1],image_shape[2])\n",
    "    cls_ids=tf.cast(cls_ids,dtype=tf.float32)\n",
    "    matched_gt_idx,positive_mask,ignore_mask=self._match_anchor_boxes(anchor_boxes,gt_boxes)\n",
    "    matched_gt_boxes=tf.gather(gt_boxes,matched_gt_idx)\n",
    "    # print(anchor_boxes, matched_gt_boxes)\n",
    "    box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
    "    matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "    cls_target=tf.where(tf.not_equal(positive_mask,1.0),-1.0,matched_gt_cls_ids)\n",
    "    cls_target=tf.where(tf.equal(ignore_mask,1.0),-2.0,cls_target)\n",
    "    cls_target=tf.expand_dims(cls_target,axis=-1)\n",
    "    label=tf.concat([box_target,cls_target],axis=-1)\n",
    "    return label\n",
    "  \n",
    "  def encode_batch(self,batch_images,gt_boxes,cls_ids):\n",
    "    images_shape=tf.shape(batch_images)\n",
    "    batch_size=images_shape[0]\n",
    "    # print(cls_ids)\n",
    "    labels=tf.TensorArray(dtype=tf.float32,size=batch_size,dynamic_size=True)\n",
    "    for i in range(batch_size):\n",
    "      label=self._encode_sample(images_shape,gt_boxes[i],cls_ids[i])\n",
    "      labels=labels.write(i,label)\n",
    "    batch_images=tf.keras.applications.resnet.preprocess_input(batch_images)\n",
    "    return batch_images,labels.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnkVhhVptJk-"
   },
   "outputs": [],
   "source": [
    "# building the net\n",
    "def get_backbone():\n",
    "  backbone=keras.applications.ResNet50(\n",
    "      include_top=False,input_shape=[None,None,3]\n",
    "  )\n",
    "  c3_output,c4_output,c5_output=[\n",
    "    backbone.get_layer(layer_name).output\n",
    "    for layer_name in [\"conv3_block4_out\",\"conv4_block6_out\",\"conv5_block3_out\"]\n",
    "  ]\n",
    "  return keras.Model(inputs=[backbone.inputs],outputs=[c3_output,c4_output,c5_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPqUxt03ceXw"
   },
   "outputs": [],
   "source": [
    "class FeaturePyramid(keras.layers.Layer):\n",
    "  def __init__(self,backbone=None,**kwargs):\n",
    "    super(FeaturePyramid,self).__init__(name=\"FeaturePyramid\",**kwargs)\n",
    "    self.backbone=backbone if backbone else get_backbone()\n",
    "    self.conv_c3_1x1=keras.layers.Conv2D(256,1,1,\"same\")\n",
    "    self.conv_c4_1x1=keras.layers.Conv2D(256,1,1,\"same\")\n",
    "    self.conv_c5_1x1=keras.layers.Conv2D(256,1,1,\"same\")\n",
    "    self.conv_c3_3x3=keras.layers.Conv2D(256,3,1,\"same\")\n",
    "    self.conv_c4_3x3=keras.layers.Conv2D(256,3,1,\"same\")\n",
    "    self.conv_c5_3x3=keras.layers.Conv2D(256,3,1,\"same\")\n",
    "    self.conv_c6_3x3=keras.layers.Conv2D(256,3,2,\"same\")\n",
    "    self.conv_c7_3x3=keras.layers.Conv2D(256,3,2,\"same\")\n",
    "    self.upsample_2x=keras.layers.UpSampling2D(2)\n",
    "\n",
    "  def call(self,images,training=False):\n",
    "    c3_output,c4_output,c5_output=self.backbone(images,training=training)\n",
    "    p3_output=self.conv_c3_1x1(c3_output)\n",
    "    p4_output=self.conv_c4_1x1(c4_output)\n",
    "    p5_output=self.conv_c5_1x1(c5_output)\n",
    "    p4_output=p4_output+self.upsample_2x(p5_output)\n",
    "    p3_output=p3_output+self.upsample_2x(p4_output)\n",
    "    p3_output=self.conv_c3_3x3(p3_output)\n",
    "    p4_output=self.conv_c4_3x3(p4_output)\n",
    "    p5_output=self.conv_c5_3x3(p5_output)\n",
    "    p6_output=self.conv_c6_3x3(c5_output)\n",
    "    p7_output=self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
    "    return p3_output,p4_output,p5_output,p6_output,p7_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wBA2mcFfK7p"
   },
   "outputs": [],
   "source": [
    "def build_head(output_filters,bias_init):\n",
    "  head=keras.Sequential([keras.Input(shape=[None,None,256])])\n",
    "  kernel_init=tf.initializers.RandomNormal(0.0,0.01)\n",
    "  for _ in range(4):\n",
    "    head.add(keras.layers.Conv2D(256,3,padding=\"same\",kernel_initializer=kernel_init))\n",
    "    head.add(keras.layers.ReLU())\n",
    "  head.add(keras.layers.Conv2D(output_filters,3,1,padding=\"same\",kernel_initializer=kernel_init,bias_initializer=bias_init,))\n",
    "  return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTBiiKGWgEHA"
   },
   "outputs": [],
   "source": [
    "class RetinaNet(keras.Model):\n",
    "  def __init__(self,num_classes=471,backbone=None,**kwargs):\n",
    "    super(RetinaNet,self).__init__(name=\"RetinaNet\",**kwargs)\n",
    "    self.fpn=FeaturePyramid(backbone)\n",
    "    self.num_classes=num_classes\n",
    "\n",
    "    prior_probability=tf.constant_initializer(-np.log((1-0.01)/0.01))\n",
    "    self.cls_head=build_head(9*num_classes,prior_probability)\n",
    "    self.box_head=build_head(9*4,\"zeros\")\n",
    "\n",
    "  def call(self,image,training=False):\n",
    "    features=self.fpn(image,training=training)\n",
    "    N=tf.shape(image)[0]\n",
    "    cls_outputs=[]\n",
    "    box_outputs=[]\n",
    "\n",
    "    for feature in features:\n",
    "      box_outputs.append(tf.reshape(self.box_head(feature),[N,-1,4]))\n",
    "      cls_outputs.append(tf.reshape(self.cls_head(feature),[N,-1,self.num_classes]))\n",
    "      cls_outputs=tf.concat(cls_outputs,axis=1)\n",
    "      box_outputs=tf.concat(box_outputs,axis=1)\n",
    "      # print(box_outputs,cls_outputs)\n",
    "      return tf.concat([box_outputs,cls_outputs],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfJtxVrMhoAr"
   },
   "outputs": [],
   "source": [
    "class DecodePredictions(tf.keras.layers.Layer):\n",
    "  def __init__(self,num_classes,confidence_threshold=0.5,max_detections_per_class=100,max_detections=100,box_variance=[0.1,0.1,0.2,0.2],**kwargs):\n",
    "    super(DecodePredictions,self).__init__(**kwargs)\n",
    "    self.num_classes=num_classes\n",
    "    self.confidence_threshold=confidence_threshold\n",
    "    self.nms_iou_threshold=nms_iou_threshold\n",
    "    self.max_detections_per_class=max_detections_per_class\n",
    "    self.max_detections=max_detections\n",
    "\n",
    "    self._anchor_box=AnchorBox()\n",
    "    self._box_variance=tf.convert_to_tensor([0.1,0.1,0.2,0.2],dtype=float32)\n",
    "\n",
    "  def _decode_box_prediction(self,anchor_boxes,box_predictions):\n",
    "    boxes=box_predictions*self._box_variance\n",
    "    boxes=tf.concat([boxes[:,:,:2]*anchore_boxes[:,:,:2]+anchor_boxes[:,:,:2],tf.math.exp(boxes[:,:,2:])*anchor_boxes[:,:,2:],],axix=-1)\n",
    "    boxes_transformed=convert_to_corners(boxes)\n",
    "    return boxes_transformed\n",
    "  \n",
    "  def call(self,images,predictions):\n",
    "    image_shape=tf.cast(tf.shape(images),dtype=tf.float32)\n",
    "    anchor_boxes=self._anchor_box.get_anchors(image_shape[1],image_shape[2])\n",
    "    box_predictions=predictions[:,:,:4]\n",
    "    cls_predictions=tf.nn.sigmoid(predictions[:,:,4:])\n",
    "    boxes=self._decode_box_prediction(anchor_boxes[None,...],box_predictions)\n",
    "\n",
    "    return tf.image.combined_non_max_suppression(tf.expand_dims(boxes,axis=2),cls_predictions,self.max_detections_per_class,self.max_detections,self.nms_iou_threshold,self.confidence_threshold,clip_boxes=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxo6NFP9mg8o"
   },
   "outputs": [],
   "source": [
    "class RetinaNetBoxLoss(tf.losses.Loss):\n",
    "  def __init__(self,delta):\n",
    "    super(RetinaNetBoxLoss,self).__init__(reduction=\"none\",name=\"RetinaNetBoxLoss\")\n",
    "    self._delta=delta\n",
    "\n",
    "  def call(self,y_true,y_pred):\n",
    "    difference=y_true-y_pred\n",
    "    absolute_difference=tf.abs(difference)\n",
    "    squared_difference=difference**2\n",
    "    loss=tf.where(tf.less(absolute_difference,self._delta),0.5*squared_difference,absolute_difference-0.5,)\n",
    "    return tf.reduce_sum(loss,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UOUsrt1nUnw"
   },
   "outputs": [],
   "source": [
    "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
    "  def __init__(self,alpha,gamma):\n",
    "    super(RetinaNetClassificationLoss,self).__init__(\n",
    "        reduction=\"none\",name=\"RetinaNetClassificationLoss\"\n",
    "    )\n",
    "    self._alpha=alpha\n",
    "    self._gamma=gamma\n",
    "\n",
    "  def call(self,y_true,y_pred):\n",
    "    cross_entropy=tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
    "    probs=tf.nn.sigmoid(y_pred)\n",
    "    alpha=tf.where(tf.equal(y_true,1.0),self._alpha,(1.0-self._alpha))\n",
    "    pt=tf.where(tf.equal(y_true,1.0),probs,1-probs)\n",
    "    loss=alpha*tf.pow(1.0-pt,self._gamma)*cross_entropy\n",
    "    return tf.reduce_sum(loss,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oK8iOPZAotTG"
   },
   "outputs": [],
   "source": [
    "class RetinaNetLoss(tf.losses.Loss):\n",
    "  def __init__(self,num_classes=471,alpha=0.25,gamma=2.0,delta=1.0):\n",
    "    super(RetinaNetLoss,self).__init__(reduction=\"auto\",name=\"RetinaNetLoss\")\n",
    "    self._clf_loss=RetinaNetClassificationLoss(alpha,gamma)\n",
    "    self._box_loss=RetinaNetBoxLoss(delta)\n",
    "    self._num_classes=num_classes\n",
    "  \n",
    "  def call(self,y_true,y_pred):\n",
    "    y_pred=tf.cast(y_pred,dtype=tf.float32)\n",
    "    box_labels=y_true[:,:,:4]\n",
    "    box_predictions=y_pred[:,:,:4]\n",
    "    cls_labels=tf.one_hot(tf.cast(y_true[:,:,4],dtype=tf.int32),depth=self._num_classes,dtype=tf.float32,)\n",
    "\n",
    "    cls_predictions=y_pred[:,:,4:]\n",
    "    positive_mask=tf.cast(tf.greater(y_true[:,:,4],-1.0),dtype=tf.float32)\n",
    "    ignore_mask=tf.cast(tf.equal(y_true[:,:,4],-2.0),dtype=tf.float32)\n",
    "    clf_loss=self._clf_loss(cls_labels,cls_predictions)\n",
    "    box_loss=self._box_loss(box_labels,box_predictions)\n",
    "    clf_loss=tf.where(tf.equal(ignore_mask,1.0),0.0,clf_loss)\n",
    "    box_loss=tf.where(tf.equal(positive_mask,1.0),box_loss,0.0)\n",
    "    normalizer=tf.reduce_sum(positive_mask,axis=-1)\n",
    "    clf_loss=tf.math.divide_no_nan(tf.reduce_sum(clf_loss,axis=-1),normalizer)\n",
    "    box_loss=tf.math.divide_no_nan(tf.reduce_sum(box_loss,axis=-1),normalizer)\n",
    "    loss=clf_loss+box_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65096,
     "status": "ok",
     "timestamp": 1621438828765,
     "user": {
      "displayName": "Camus Shen",
      "photoUrl": "",
      "userId": "08908240325552421641"
     },
     "user_tz": -480
    },
    "id": "R5zSvBpMSAj0",
    "outputId": "4fd460f0-76bc-4a08-89be-62cb8706bc6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qB7EodoBQJfF"
   },
   "outputs": [],
   "source": [
    "f=open('drive/MyDrive/cv/hw2/categories.json')\n",
    "categorires=json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1044,
     "status": "ok",
     "timestamp": 1621444142640,
     "user": {
      "displayName": "Camus Shen",
      "photoUrl": "",
      "userId": "08908240325552421641"
     },
     "user_tz": -480
    },
    "id": "es0xzgAKNCSU",
    "outputId": "012349e2-b24d-4719-b168-d104d41c09d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427\n"
     ]
    }
   ],
   "source": [
    "model_dir=\"retinanet\"\n",
    "label_encoder=LabelEncoder()\n",
    "\n",
    "num_classes=len(categorires)\n",
    "# num_classes=427\n",
    "print(num_classes)\n",
    "batch_size=50\n",
    "\n",
    "learning_rates=[2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "learning_rate_boundaries=[125, 250, 500, 240000, 360000]\n",
    "learning_rate_fn=tf.optimizers.schedules.PiecewiseConstantDecay(boundaries=learning_rate_boundaries,values=learning_rates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3742,
     "status": "ok",
     "timestamp": 1621444149223,
     "user": {
      "displayName": "Camus Shen",
      "photoUrl": "",
      "userId": "08908240325552421641"
     },
     "user_tz": -480
    },
    "id": "fFOahleVOEx3",
    "outputId": "93da6608-8fb5-488a-cf30-c778d676d7fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Verbose\n"
     ]
    }
   ],
   "source": [
    "%xmode Verbose\n",
    "resnet50_backbone=get_backbone()\n",
    "loss_fn=RetinaNetLoss(num_classes)\n",
    "model=RetinaNet(num_classes,resnet50_backbone)\n",
    "\n",
    "optimizer=tf.optimizers.SGD(learning_rate=learning_rate_fn,momentum=0.9)\n",
    "model.compile(loss=loss_fn,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiAhIDjuOfeU"
   },
   "outputs": [],
   "source": [
    "callbacks=[\n",
    "  tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=os.path.join(model_dir,\"weights\"+\"_epoch_{epoch}\"),\n",
    "      monitor=\"loss\",\n",
    "      save_best_only=False,\n",
    "      save_weights_only=True,\n",
    "      verbose=1,\n",
    "  )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDpQawRuoJUJ"
   },
   "outputs": [],
   "source": [
    "train_data=tf.data.TFRecordDataset(\"drive/MyDrive/train.tfrecord\")\n",
    "val_data=tf.data.TFRecordDataset(\"drive/MyDrive/cv/hw2/val.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_F6TUWysn2F6"
   },
   "outputs": [],
   "source": [
    "features = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/class/text': tf.io.VarLenFeature(tf.string),\n",
    "        'image/object/class/label': tf.io.VarLenFeature(tf.int64),         \n",
    "        }\n",
    "def _parse_function(proto):\n",
    "  \n",
    "  \n",
    "  # Load one example\n",
    "  parsed_features = tf.io.parse_single_example(proto, features)\n",
    "  height = tf.cast(parsed_features['image/height'], tf.int32)\n",
    "  width = tf.cast(parsed_features['image/width'], tf.int32)\n",
    "  # depth = tf.cast(parsed_features['image/depth'], tf.int32)\n",
    "  # # Turn your saved image string into an array\n",
    "\n",
    "  # image_shape = tf.parallel_stack([height, width,3])\n",
    "  # bboxes_shape=tf.parallel_stack([-1,4])\n",
    "  image= tf.io.decode_raw(\n",
    "      parsed_features['image/encoded'], tf.uint8)\n",
    "  # print(image)\n",
    "  image = tf.reshape(image, [height, width,3])\n",
    "  # print(image)\n",
    "  \n",
    "  bbox=tf.stack([\n",
    "    tf.sparse.to_dense(parsed_features['image/object/bbox/xmin']),\n",
    "    tf.sparse.to_dense(parsed_features['image/object/bbox/ymin']),\n",
    "    tf.sparse.to_dense(parsed_features['image/object/bbox/xmax']),\n",
    "    tf.sparse.to_dense(parsed_features['image/object/bbox/ymax']),\n",
    "  ],\n",
    "    axis=-1\n",
    "  )\n",
    "\n",
    "  class_id=tf.sparse.to_dense(tf.cast(parsed_features['image/object/class/label'],tf.int32))\n",
    "  image, bbox = random_flip_horizontal(image, bbox)\n",
    "  image, image_shape, _ = resize_and_pad_image(image)\n",
    "  bbox = tf.stack(\n",
    "    [\n",
    "        bbox[:, 0] * image_shape[1],\n",
    "        bbox[:, 1] * image_shape[0],\n",
    "        bbox[:, 2] * image_shape[1],\n",
    "        bbox[:, 3] * image_shape[0],\n",
    "    ],\n",
    "    axis=-1,\n",
    "  )\n",
    "  bbox = convert_to_xywh(bbox)\n",
    "#   print(image.numpy())\n",
    "  # print(image,bbox,class_id)\n",
    "  return image,bbox,class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "executionInfo": {
     "elapsed": 3986,
     "status": "error",
     "timestamp": 1621443795200,
     "user": {
      "displayName": "Camus Shen",
      "photoUrl": "",
      "userId": "08908240325552421641"
     },
     "user_tz": -480
    },
    "id": "9SqbRth2cqum",
    "outputId": "b9a27b57-f864-4e9a-fec6-87437a07b004"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9bdeb704214b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/train.tfrecord\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mautotune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_parse_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautotune\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Choose a value of `max_elems` that is at least as large as the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_parse_function' is not defined"
     ]
    }
   ],
   "source": [
    "train_data=tf.data.TFRecordDataset(\"drive/MyDrive/train.tfrecord\")\n",
    "autotune=tf.data.experimental.AUTOTUNE\n",
    "train_data=train_data.map(_parse_function,num_parallel_calls=autotune)\n",
    "\n",
    "print(train_data.shape())#ParallelMapDataset\n",
    "train_data=train_data.shuffle(1000*batch_size)\n",
    "# print(train_data.shape())#ShuffleDataset\n",
    "train_data=train_data.padded_batch(\n",
    "    batch_size=20,padding_values=(0.0,1e-8,-1),drop_remainder=True\n",
    ")\n",
    "# print(train_data.shape())#PaddedBatchDataset\n",
    "train_data=train_data.map(\n",
    "  label_encoder.encode_batch,num_parallel_calls=autotune    \n",
    ")\n",
    "# print(type(train_data))# <class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'>\n",
    "train_data=train_data.apply(tf.data.experimental.ignore_errors())\n",
    "# print(type(train_data))# <class 'tensorflow.python.data.experimental.ops.error_ops._IgnoreErrorsDataset'>\n",
    "train_data=train_data.prefetch(autotune)\n",
    "# print(type(train_data))# <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
    "val_data=val_data.map(_parse_function,num_parallel_calls=autotune)\n",
    "val_data=val_data.padded_batch(\n",
    "    batch_size=1,padding_values=(0.0,1e-8,-1),drop_remainder=True\n",
    ")\n",
    "val_data=val_data.map(label_encoder.encode_batch,num_parallel_calls=autotune)\n",
    "val_data=val_data.apply(tf.data.experimental.ignore_errors())\n",
    "val_data=val_data.prefetch(autotune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxqXHbJbjnlK"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "executionInfo": {
     "elapsed": 13297,
     "status": "error",
     "timestamp": 1621440241421,
     "user": {
      "displayName": "Camus Shen",
      "photoUrl": "",
      "userId": "08908240325552421641"
     },
     "user_tz": -480
    },
    "id": "oY_vTx-9d-ns",
    "outputId": "cba21d9a-b955-44ff-9157-828d025f02e4"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-add7391c997e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model.fit(train_data,validation_data=val_data,epochs=epochs,callbacks=callbacks,verbose=1,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mmodel.fit\u001b[0m \u001b[0;34m= <bound method Model.fit of <__main__.RetinaNet object at 0x7f29dd4647d0>>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mx\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mtrain_data\u001b[0m \u001b[0;34m= <PrefetchDataset shapes: ((20, None, None, 3), (None, None, 5)), types: (tf.float32, tf.float32)>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mbatch_size\u001b[0m \u001b[0;34m= 50\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mvalidation_data\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mepochs\u001b[0m \u001b[0;34m= 1\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mcallbacks\u001b[0m \u001b[0;34m= [<tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f29dcc97590>]\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mverbose\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self=<__main__.RetinaNet object>, x=<PrefetchDataset shapes: ((20, None, None, 3), (None, None, 5)), types: (tf.float32, tf.float32)>, y=None, batch_size=1, epochs=1, verbose=1, callbacks=<tensorflow.python.keras.callbacks.CallbackList object>, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expect x to be a non-empty array or dataset.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mValueError\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expect x to be a non-empty array or dataset."
     ]
    }
   ],
   "source": [
    "epochs=1\n",
    "print(tf.__version__)\n",
    "model.fit(train_data,validation_data=val_data,epochs=epochs,callbacks=callbacks,verbose=1,)\n",
    "# model.fit(x=whole_dataset_arrays,batch_size=4,validation_data=None,epochs=epochs,callbacks=callbacks,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIX88wWyeorg"
   },
   "outputs": [],
   "source": [
    "weights_dir=\"data\"\n",
    "\n",
    "latest_checkpoint=tf.train.latest_checkpoint(weights_dir)\n",
    "model.load_weights(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLP-HJt5e6g6"
   },
   "outputs": [],
   "source": [
    "image=tf.keras.Input(shape=[None,None,3],name=\"image\")\n",
    "predictions=model(image,training=False)\n",
    "detections=DecodePredictions(confidence_threshold=0.5)(image,predictions)\n",
    "inference_model=tf.keras.Model(inputs=image,outputs=detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY9qhwhffX-G"
   },
   "outputs": [],
   "source": [
    "def prepare_img(image):\n",
    "  image,_,ratio=resize_and_pad_image(image,jitter=None)\n",
    "  image=tf.keras.applications.resnet.preprocess_input(image)\n",
    "  return tf.expand_dims(image,axis=0),ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64ITIiAFfng9"
   },
   "outputs": [],
   "source": [
    "val_dataset=tfds.load()\n",
    "int2str=dataset_info.features[\"objects\"][\"label\"].int2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zxv3GN_hfygm"
   },
   "outputs": [],
   "source": [
    "for sample in val_dataset.take(2):\n",
    "  image=tf.cast(sample[\"image\"],dtype=tf.float32)\n",
    "  input_image,ratio=prepare_image(image)\n",
    "  detections=inference_model.predict(input_image)\n",
    "  num_detections=detecitions.valid_detections[0]\n",
    "  class_names=[int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]]\n",
    "  visualize_detections(image,detections.nmsed_boxes[0][:num_detections]/ratio,class_names,detections.nmsed_scores[0][:num_detections],)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "“““cv21.ipynb”的副本”的副本”的副本",
   "provenance": [
    {
     "file_id": "1OVhve5MRiX954nGBvtKkSNg_UF7Xwtqr",
     "timestamp": 1621476967856
    },
    {
     "file_id": "15FaPyvb19wPrEUtCse0jqUYaDuunFgJh",
     "timestamp": 1621444436776
    },
    {
     "file_id": "1qpGPFT-9KjduOqBLe1HwyMLm7d10Gaqi",
     "timestamp": 1621428560707
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
